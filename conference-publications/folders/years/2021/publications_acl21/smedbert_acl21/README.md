# SMedBERT: A Knowledge-Enhanced Pre-trained Language Model with Structured Semantics for Medical Text Mining

```
@inproceedings{smedbert_acl21,
title = {{SM}ed{BERT}: A Knowledge-Enhanced Pre-trained Language Model with Structured Semantics for Medical Text Mining},
author = {Zhang, Taolin and Cai, Zerui and Wang, Chengyu and Qiu, Minghui and Yang, Bite and He, Xiaofeng},
booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (ACL-IJCNLP)},
pages = {5882--5893},
year = {2021}
}
```

links
- [acl](https://aclanthology.org/2021.acl-long.457)
