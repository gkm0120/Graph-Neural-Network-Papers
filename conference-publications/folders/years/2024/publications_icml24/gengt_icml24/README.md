# What Improves the Generalization of Graph Transformers? A Theoretical Dive into the Self-attention and Positional Encoding

```
@inproceedings{gengt_icml24,
title = {What Improves the Generalization of Graph Transformers? {A} Theoretical Dive into the Self-attention and Positional Encoding},
author = {Li, Hongkang and Wang, Meng and Ma, Tengfei and Liu, Sijia and Zhang, Zaixi and Chen, Pin-Yu},
booktitle = {Proceedings of the 41st International Conference on Machine Learning (ICML)},
pages = {28784--28829},
year = {2024}
}
```

links
- [icml](https://icml.cc/Conferences/2024/Schedule?showEvent=33179)
- [pmlr](https://proceedings.mlr.press/v235/li24bo.html)
- [openreview](https://openreview.net/forum?id=mJhXlsZzzE)
